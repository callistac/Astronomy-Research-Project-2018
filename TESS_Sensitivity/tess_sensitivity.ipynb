{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESS Sensitivity Analysis\n",
    "\n",
    "__If you make use of this code, please cite Christ, C. N., Montet, B. T., & Fabrycky, D. C. 2018, arXiv:1810.02826__\n",
    "\n",
    "The following code is used to determine how many and what types of planets TESS will be most sensitive to in the Kepler field for a given number of sectors\n",
    "\n",
    "This analysis makes use of data from the NASA Exoplanet Archive as well as the following papers: Sullivan et al. 2015 and Christiansen et al. 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from scipy.special import erf\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in Data frame the NASA Exoplanet Archive\n",
    "df = pd.read_csv('exoplanet_archive_complete_version4.csv', skiprows=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#including only the following columns/data:\n",
    "df = df.filter(items=['rowid', 'kepid', 'kepoi_name', 'kepler_name', 'koi_disposition', 'koi_depth', 'koi_duration', 'koi_prad', 'koi_period', 'koi_period_err1', 'koi_time0bk', 'koi_time0bk_err1', 'koi_smass', 'koi_srad', 'koi_sma', 'koi_eccen', 'koi_incl', 'koi_longp'])\n",
    "\n",
    "#adding in columns that we will later have data for\n",
    "df['kep_contam_ratio'] = pd.Series(np.nan, index=df.index)\n",
    "df['tess_contam_ratio'] = pd.Series(np.nan, index=df.index)\n",
    "df['new_transit_depth'] = pd.Series(np.nan, index=df.index)\n",
    "df['tess_magnitude'] = pd.Series(np.nan, index=df.index)\n",
    "df['tot_noise'] = pd.Series(np.nan, index=df.index)\n",
    "df['SNR1_1'] = pd.Series(np.nan, index=df.index)\n",
    "df['SNR2_1'] = pd.Series(np.nan, index=df.index)\n",
    "df['SNR1_2'] = pd.Series(np.nan, index=df.index)\n",
    "df['SNR2_2'] = pd.Series(np.nan, index=df.index)\n",
    "\n",
    "#falsely labeled planet\n",
    "pos1 = np.where(df['kepid'] == 8110757)\n",
    "df.at[6993, 'koi_disposition'] = 'FALSE POSITIVE'\n",
    "\n",
    "#only using confirmed or candidate systems\n",
    "df = df.loc[df['koi_disposition'].isin(['CONFIRMED', 'CANDIDATE'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kepler contamination ratios\n",
    "file1 = open('kepler_fov_search.txt')\n",
    "df2 = pd.read_csv(file1, names=[ 'Kepler_ID', 'Crowding_season_0', 'Crowding_season_1', 'Crowding_season_2', 'Crowding_season_3'], skiprows=2)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recording systems missing from the kepler contam file (should be none)\n",
    "#taking median value of contam ratio and inputting in dataframe for kepler\n",
    "missing_systems = []\n",
    "for index, row in df.iterrows():\n",
    "    idnum = row['kepid']\n",
    "    loc = np.where(df2['Kepler_ID'] == idnum)\n",
    "    if (len(loc[0]) == 0):\n",
    "        missing_systems.append(idnum)\n",
    "    else:\n",
    "        rowdf2 = df2.iloc[loc[0]]\n",
    "        crowding_array = np.array([rowdf2.iloc[0]['Crowding_season_0'], rowdf2.iloc[0]['Crowding_season_1'], \n",
    "                                   rowdf2.iloc[0]['Crowding_season_2'], rowdf2.iloc[0]['Crowding_season_3']])\n",
    "        contam_val = np.nanmedian(crowding_array)\n",
    "        df.at[index, 'kep_contam_ratio'] = contam_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tess contamination ratio files \n",
    "df3 = pd.read_csv('MAST_Crossmatch_TIC.csv', skiprows=4)\n",
    "df4 = pd.read_csv('MAST_Crossmatch_TIC_missing_Tmag.csv', skiprows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- recording missing systems from tess contam file (should be a fair amount) \n",
    "- search for those missing systems in another dataset and input the tess magnitude/contamratio into df\n",
    "- if not missing, input tess mag/contamratio into df\n",
    "- should have no missing_systems2 after running this code\n",
    "'''\n",
    "missing_systems2 = []\n",
    "nan_systems = []\n",
    "for index, row in df.iterrows():\n",
    "    idnum = row['kepid']\n",
    "    loc = np.where(df3['Kepler_ID'] == idnum)\n",
    "    if (len(loc[0]) == 0):\n",
    "        loc1 = np.where(df4['Kepler_ID'] == idnum)\n",
    "        if (len(loc1) == 0):\n",
    "            missing_systems2.append(idnum)\n",
    "        else: \n",
    "            row2df = df4.iloc[(loc1[0][0])]\n",
    "            missing_tmag = row2df['Tmag']\n",
    "            missing_contamratio = row2df['contratio']\n",
    "            if np.isnan(missing_contamratio):\n",
    "                nan_systems.append(idnum)\n",
    "                df.at[index, 'tess_magnitude'] = missing_tmag\n",
    "                df.at[index, 'tess_contam_ratio'] = 0.0\n",
    "            else:\n",
    "                df.at[index, 'tess_magnitude'] = missing_tmag\n",
    "                df.at[index, 'tess_contam_ratio'] = missing_contamratio\n",
    "    else:\n",
    "        rowdf2 = df3.iloc[loc[0]]\n",
    "    \n",
    "        contam_val = rowdf2.iloc[0]['contratio']\n",
    "        tess_mag = rowdf2.iloc[0]['Tmag']\n",
    "        \n",
    "        if (np.isnan(contam_val) == True):\n",
    "            nan_systems.append(idnum)\n",
    "            contam_val = 0.0\n",
    "            \n",
    "        df.at[index, 'tess_contam_ratio'] = contam_val\n",
    "        df.at[index, 'tess_magnitude'] = tess_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(missing_systems2)\n",
    "#print('\\n')\n",
    "#print(nan_systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking the observed depth and dividing by the kepler contam ratio to find the true transit depth\n",
    "#contaminating transit depts with the tess contam ratio (first have to convert t_contam_ratio to the same form as k_contam_ratio)#input new transit depth into df\n",
    "for index, row in df.iterrows():\n",
    "    koidepth = row['koi_depth'] \n",
    "    k_contam_ratio = row['kep_contam_ratio']\n",
    "    t_contam_ratio = row['tess_contam_ratio']\n",
    "    #koidepth = koidepth / k_contam_ratio\n",
    "    t_contam_ratio = 1 / (1 + t_contam_ratio)\n",
    "    koidepth = koidepth * t_contam_ratio  \n",
    "    df.at[index, 'new_transit_depth'] = koidepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coming up with the equation that will convert a tess magnitude to total noise by using plot in Sullivan et al. 2015\n",
    "for index, row in df.iterrows():\n",
    "    mag = row['tess_magnitude']\n",
    "    \n",
    "    star_noise_log = 0.2235*mag + 0.0565\n",
    "    sky_noise_log = 0.3347*mag - 1.6776\n",
    "    read_noise_log = 0.3486*mag - 1.940\n",
    "    syst_noise_log = np.log10(60)\n",
    "    \n",
    "    star_noise_r = 10**star_noise_log\n",
    "    sky_noise_gn = 10**sky_noise_log\n",
    "    read_noise_gy = 10**read_noise_log\n",
    "    syst_noise_b = 10**syst_noise_log\n",
    "    \n",
    "    tot_noise = np.sqrt(star_noise_r**2 + sky_noise_gn**2 + read_noise_gy**2 + syst_noise_b**2)\n",
    "    tot_noise *= np.sqrt(2)\n",
    "    df.at[index, 'tot_noise'] = tot_noise \n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function that calculates signal-to-noise ratio and inputs those SNR to our dataframe \n",
    "\n",
    "Input: \n",
    "    num_camp - the number of campaigns that TESS will observe the Kepler field (1=27.4 days, 2=54.8 days)\n",
    "Output: \n",
    "    none, adds SNR to df\n",
    "'''\n",
    "\n",
    "def Calc_SNR(num_camp):\n",
    "    for index, row in df.iterrows():\n",
    "        signal = row['new_transit_depth']\n",
    "        noise = row['tot_noise']\n",
    "        tdur_hr = row['koi_duration']\n",
    "        texp = 30 #minutes\n",
    "        period = row['koi_period']\n",
    "        tdur = tdur_hr * 60 \n",
    "        \n",
    "        #calculating the number of transits observed in a given period and number of campaigns\n",
    "        num_transits = 27.4*num_camp / period\n",
    "        N = int(num_transits)\n",
    "        \n",
    "        #the probability of detecting N+1 transits\n",
    "        prob_nplus1 = num_transits%1\n",
    "        df.at[index, 'prob_nplus1_'+ str(num_camp)] = prob_nplus1\n",
    "        \n",
    "        #SNR1 is the sig-noise ratio for detecting N transits\n",
    "        SNR1 = (signal / noise) * np.sqrt(tdur / texp) * np.sqrt(N)\n",
    "        df.at[index, 'SNR1_'+ str(num_camp)] = SNR1\n",
    "    \n",
    "        #SNR2 is the sig-noise tratio for detecting N+1 transits\n",
    "        SNR2 = (signal / noise) * np.sqrt(tdur / texp) * np.sqrt(N+1)\n",
    "        df.at[index, 'SNR2_'+ str(num_camp)] = SNR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates SNR for 1 or 2 campaigns (or specified value of campaigns (sectors), see Table 1 in Christ et al.)\n",
    "for i in range(1, 3):\n",
    "    Calc_SNR(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for converting noise to a probability of detection taken from Christiansen et al. 2015\n",
    "def Phi(x,mu=0,sigma=1):\n",
    "    t = erf((x-mu)/(sigma*sqrt(2)))\n",
    "    return 0.5 + 0.5*t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking SNR for N and N+1 for both campaigns and converting them to probabilities of detection\n",
    "#TESS has a 7.1 signal detection threshold, but best case scenario would be a 3 sigma signal detection threshold\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    snratio1_camp1 = row['SNR1_1']\n",
    "    snratio2_camp1 = row['SNR2_1']\n",
    "    snratio1_camp2 = row['SNR1_2']\n",
    "    snratio2_camp2 = row['SNR2_2']\n",
    "    prob_num_transits1 = row['prob_nplus1_1']\n",
    "    prob_num_transits2 = row['prob_nplus1_2']\n",
    "    \n",
    "    #this line was used to find total SNR for harvard people\n",
    "    df.at[index, 'total_SNR1'] = (snratio1_camp1*(1 - prob_num_transits1)) + (snratio2_camp1*(prob_num_transits1))\n",
    "    df.at[index, 'total_SNR2'] = (snratio1_camp2*(1 - prob_num_transits2)) + (snratio2_camp2*(prob_num_transits2))\n",
    "    \n",
    "    period = row['koi_period']\n",
    "    prob1_camp1 = Phi(snratio1_camp1, mu=3.0, sigma=1.0) #mu = signal detection threshold, using either 3 or 7.1\n",
    "    prob2_camp1 = Phi(snratio2_camp1, mu=3.0, sigma=1.0)\n",
    "    \n",
    "    prob1_camp2 = Phi(snratio1_camp2, mu=3.0, sigma=1.0)\n",
    "    prob2_camp2 = Phi(snratio2_camp2, mu=3.0, sigma=1.0)\n",
    "    \n",
    "    tot_prob_camp1 = prob1_camp1*(1 - prob_num_transits1) + prob2_camp1*(prob_num_transits1)\n",
    "    tot_prob_camp2 = prob1_camp2*(1 - prob_num_transits2) + prob2_camp2*(prob_num_transits2)\n",
    "    \n",
    "    df.at[index, 'tot_prob_camp1'] = tot_prob_camp1\n",
    "    df.at[index, 'tot_prob_camp2'] = tot_prob_camp2\n",
    "    \n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding out how many of these prob vals are nan\n",
    "nan_df = df['tot_prob_camp1'][~np.isfinite(df['tot_prob_camp1'])]\n",
    "print(len(nan_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding any planets that have either a missing tess magnitude or depth\n",
    "missing_tess_mag = []\n",
    "missing_depth = []\n",
    "for index, row in df.iterrows():\n",
    "    if index in list(nan_df.index.values):\n",
    "        if (np.isnan(df.loc[index]['koi_depth'])):\n",
    "            missing_depth.append(df.loc[index]['kepid'])\n",
    "        if (np.isnan(df.loc[index]['tess_magnitude'])):\n",
    "            missing_tess_mag.append(df.loc[index]['kepid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"missing tess mag\", missing_tess_mag)\n",
    "print(\"missing depth\", missing_depth)\n",
    "for kk in range(len(missing_tess_mag)):\n",
    "    if missing_tess_mag[kk] in missing_depth:\n",
    "        print(missing_tess_mag[kk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if a planet is missing a depth, then remove it\n",
    "for index, row in df.iterrows():\n",
    "    if row['kepid'] in missing_depth:\n",
    "        df = df.drop(labels=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrote this code to make sure all the planets with missing magnitudes and/or depths are erased from df\n",
    "missing_tess_mag = []\n",
    "missing_depth = []\n",
    "for index, row in df.iterrows():\n",
    "    if index in list(nan_df.index.values):\n",
    "        print(df.loc[index])\n",
    "        if (np.isnan(df.loc[index]['koi_depth'])):\n",
    "            missing_depth.append(df.loc[index]['kepid'])\n",
    "        if (np.isnan(df.loc[index]['tess_magnitude'])):\n",
    "            missing_tess_mag.append(df.loc[index]['kepid'])\n",
    "        print('\\n')\n",
    "        \n",
    "print(\"missing tess mag\", missing_tess_mag)\n",
    "print(\"missing depth\", missing_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the distribution of probabilities \n",
    "plt.hist(df['tot_prob_camp1'], np.arange(0, 1.001, 0.01))\n",
    "plt.ylim(0, 150)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(df['tot_prob_camp2'], np.arange(0, 1.001, 0.01))\n",
    "plt.ylim(0, 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following systems were of interest to a research group \n",
    "'''\n",
    "array3 = np.where(df['kepoi_name'] == 'K00500.02')\n",
    "indexval3 = array3[0][0]\n",
    "df.iloc[indexval3]\n",
    "\n",
    "array4 = np.where(df['kepoi_name'] == 'K00500.03')\n",
    "indexval4 = array4[0][0]\n",
    "df.iloc[indexval4]\n",
    "\n",
    "array5 = np.where(df['kepoi_name'] == 'K00500.04')\n",
    "indexval5 = array5[0][0]\n",
    "df.iloc[indexval5]\n",
    "\n",
    "array6 = np.where(df['kepoi_name'] == 'K00500.05')\n",
    "indexval6 = array6[0][0]\n",
    "df.iloc[indexval6]\n",
    "\n",
    "array1 = np.where(df['kepid'] == 1161345)\n",
    "print(array1)\n",
    "indexval = array1[0][0]\n",
    "print(df.iloc[indexval])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilites of detection for KOI142\n",
    "array1 = np.where(df['kepid'] == 5446285)\n",
    "print(array1)\n",
    "indexval = array1[0][0]\n",
    "print(df.iloc[indexval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prob1 = df[df['tot_prob_camp1'] >= 0.5]\n",
    "df_prob2 = df[df['tot_prob_camp2'] >= 0.5]\n",
    "\n",
    "print(\"total number of detections in 1 sector\", len(df_prob1))\n",
    "print(\"total number of detections in 2 sectors\", len(df_prob2))\n",
    "\n",
    "confirmed_df_prob1 = df_prob1[df_prob1['koi_disposition'] != 'CANDIDATE']\n",
    "candidate_df_prob1 = df_prob1[df_prob1['koi_disposition'] != 'CONFIRMED']\n",
    "confirmed_df_prob2 = df_prob2[df_prob2['koi_disposition'] != 'CANDIDATE']\n",
    "candidate_df_prob2 = df_prob2[df_prob2['koi_disposition'] != 'CONFIRMED']\n",
    "\n",
    "#print statements are to see how many confirmed/candidate planets have a chance of being detected\n",
    "print(\"confirmed (1 sector)\", len(confirmed_df_prob1))\n",
    "print(\"candidate (1 sector)\", len(candidate_df_prob1))\n",
    "\n",
    "print('\\n')\n",
    "print(\"confirmed (2 sectors)\", len(confirmed_df_prob2))\n",
    "print(\"candidate (2 sectors)\", len(candidate_df_prob2))\n",
    "\n",
    "#analyzing the red datapoint that has a small radius/period in below plot (this planet orbits a super small star)\n",
    "unique_system = df_prob1[df_prob1['koi_period'] <= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding planets that will be detected that are less than 4 Earth radii\n",
    "print(\"1 sector\", len(df_prob1[df_prob1['koi_prad'] < 4])) \n",
    "print(\"2 sector\", len(df_prob2[df_prob2['koi_prad'] < 4]))\n",
    "print('\\n')\n",
    "\n",
    "print(\"confirmed, 1 sector\", len(confirmed_df_prob1[confirmed_df_prob1['koi_prad'] < 4]))\n",
    "print(\"candidate, 1 sector\", len(candidate_df_prob1[candidate_df_prob1['koi_prad'] < 4]))\n",
    "print('\\n')\n",
    "\n",
    "print(\"confirmed, 2 sector\", len(confirmed_df_prob2[confirmed_df_prob2['koi_prad'] < 4]))\n",
    "print(\"candidate, 2 sector\", len(candidate_df_prob2[candidate_df_prob2['koi_prad'] < 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at how many detectable nan_systems we have (nan systems being that they have a nan tess contam ratio)\n",
    "detectable_nan_systems_camp1 = []\n",
    "detectable_nan_systems_camp2 = []\n",
    "\n",
    "print(len(nan_systems))\n",
    "\n",
    "for index, row in df_prob1.iterrows():\n",
    "    if row['kepid'] in nan_systems:\n",
    "        detectable_nan_systems_camp1.append(row['kepid'])\n",
    "\n",
    "detectable_nan_systems_camp2 = []\n",
    "for index, row in df_prob2.iterrows():\n",
    "    if row['kepid'] in nan_systems:\n",
    "        detectable_nan_systems_camp2.append(row['kepid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detectable_nan_systems_camp1)\n",
    "print(detectable_nan_systems_camp2)\n",
    "print(len(detectable_nan_systems_camp1))\n",
    "print(len(detectable_nan_systems_camp2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "\n",
    "The following code produces plots in order to better visualize/understand our results in the previous analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.linewidth']=2\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['ytick.major.width'] = 1\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.minor.width'] = 1\n",
    "plt.rc('xtick.major', size=6, pad=8)\n",
    "plt.rc('xtick.minor', size=4, pad=5)\n",
    "plt.rc('ytick.major', size=6, pad=8)\n",
    "plt.rc('ytick.minor', size=4, pad=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making graph of all planets detectable by kepler vs what planets will be detectable in one or two sectors with TESS\n",
    "fig, axes = plt.subplots(figsize=(7, 5))\n",
    "axes.plot((df['koi_period']), (df['koi_prad']), 'k.')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "axes.plot(df_prob2['koi_period'], df_prob2['koi_prad'], color='aqua', marker='.', linewidth=0)\n",
    "axes.plot(df_prob1['koi_period'], df_prob1['koi_prad'], 'r.')\n",
    "\n",
    "plt.xlabel('Orbital Period (days)', fontsize=14)\n",
    "plt.ylabel(r'Planet Radius $(R_e)$', fontsize=14)\n",
    "\n",
    "red_patch = mpatches.Patch(color='red', label='One Sector')\n",
    "blue_patch = mpatches.Patch(color='aqua', label='Two Sector')\n",
    "axes.legend(handles=[red_patch, blue_patch])\n",
    "\n",
    "divider = make_axes_locatable(axes)\n",
    "axHistx = divider.append_axes(\"top\", 1.0, pad=0.0, sharex=axes)\n",
    "axHisty = divider.append_axes(\"right\", 1.0, pad=0.0, sharey=axes)\n",
    "\n",
    "axHistx.xaxis.set_tick_params(labelbottom=False, direction='inout')\n",
    "axHisty.yaxis.set_tick_params(labelleft=False, direction='inout')\n",
    "\n",
    "logbins = np.geomspace(0.1, 1000, 13) \n",
    "axHistx.hist(df_prob2['koi_period'], bins=logbins, edgecolor='k', facecolor='w')\n",
    "\n",
    "logbins2 = np.geomspace(0.1, 1000, 13) #df_prob2['koi_prad'].min(), df_prob2['koi_prad'].max(), 20)\n",
    "axHisty.hist(df_prob2['koi_prad'], bins=logbins2, edgecolor='k', facecolor='w', orientation='horizontal', log=True)\n",
    "\n",
    "plt.gcf().subplots_adjust(left=0.17, bottom=0.17, right=0.94, top=0.94, wspace=0.0, hspace=0.0)\n",
    "\n",
    "plt.ylim(ymax=2*10**2)\n",
    "\n",
    "#plt.savefig('period_vs_radius_confirm&cand3.0_poster.pdf', transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving big df with all planets in it\n",
    "df.to_csv('tess_sensitivity_inital_analysis7_sigma.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving df_prob2 \n",
    "df_prob2.to_csv('prob_of_detection_camp2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dataframe to include in paper as well as machine readable table\n",
    "df_prob2 = pd.read_csv('prob_of_detection_camp2.csv')\n",
    "sensdf_for_paper = df_prob2[['kepoi_name', 'kepler_name', 'tess_magnitude', 'new_transit_depth', 'tot_noise', 'total_SNR1', 'total_SNR2', 'tot_prob_camp1', 'tot_prob_camp2']]\n",
    "sensdf_for_paper = sensdf_for_paper.sort_values(by=['kepoi_name'])\n",
    "sensdf_for_paper = sensdf_for_paper.round({'tess_magnitude':2, 'new_transit_depth':0, 'tot_noise':0, 'total_SNR1':2, 'total_SNR2':2, 'tot_prob_camp1':3, 'tot_prob_camp2':3})\n",
    "#sensdf_for_paper.to_csv('tess_sensitivity_final_table.csv', index = False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recreating plot from sullivan et al. where the coefficients were used in the TESS Sensitivity Analysis\n",
    "x = np.arange(5, 20, 2)\n",
    "syst_noise = np.zeros(shape=(8))\n",
    "syst_noise += np.log10(60)\n",
    "star_noise_log = 0.2235*x + 0.0565\n",
    "sky_noise_log = 0.3347*x - 1.6776\n",
    "read_noise_log = 0.3486*x - 1.940\n",
    "syst_noise_log = np.log10(60)\n",
    "\n",
    "star_noise_r = 10**star_noise_log\n",
    "sky_noise_gn = 10**sky_noise_log\n",
    "read_noise_gy = 10**read_noise_log\n",
    "syst_noise_b = 10**syst_noise_log\n",
    "    \n",
    "tot_noise = np.sqrt(star_noise_r**2 + sky_noise_gn**2 + read_noise_gy**2 + syst_noise_b**2)\n",
    "plt.plot(x, (0.2235*x + 0.0565), 'r')\n",
    "plt.plot(x, (0.3347*x - 1.6776), 'g-')\n",
    "plt.plot(x, (0.3486*x - 1.940), 'y-')\n",
    "plt.plot(x, syst_noise, 'b')\n",
    "plt.plot(x, np.log10(tot_noise), 'k')\n",
    "\n",
    "plt.axis([5, 18, 1, 5])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
